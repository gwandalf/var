\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}

\title{Your Paper}

\author{You}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section{Abstract of the paper}

This paper presents a Human Machine multimodal Interface framework for Virtual Environments (VE). The addressed interaction paradigms are based on speech and gesture recognition (multimodal aspect). Its main challenge is to be efficient in a highly interactive context, with an obligation to adapt to real-time. Another requirement is to be an adaptable tool, re-usable, extensible and configurable.

A multimodal interface use recognition methods that must take into account the VE context. For example, in speech recognition, spatial positioning depends on the spatial position of the speaker (\textit{"The left object"} refers to different objects with respect to the user position in the scene). The problem can be solved by implementing a close coupling between the scene representation and the recognition modules. However, such a coupling suffers of 3 main issues (TODO: reformuler apr√®s avoir mieux compris l'article):
\begin{enumerate}
	\item Sampling rates policies differ between classic speech recognition methods and VR systems.
	\item De-synchronization between the recognition system and the VE. The recognition process needs information on the current state of the scene and so "waits" for new data, while the systems evolves through another state. There is also a need for former information, not present any more in the current state of the scene.
	\item The simulation rate can change.
\end{enumerate}

\subsection{Overview of the system}
\label{sec:overview}
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{res/overview.png}
\caption{\label{fig:overview}Architecture of the framework.}
\end{figure}

The figure \ref{fig:overview} shows an overview of the framework architecture. It includes three main modules: \textit{gesture processing} (PrOSA), \textit{multimodal integration and analysis} (tATN) and \textit{Knowledge Representation Layer} (KRL). Their purpose will be explained in details in the latter sections of the report. However, some generalities can be given here:
\begin{description}
	\item[PrOSA:] it uses attribute sequences generated by the scene, interpreting them as gestures. The direct effects on the VE are produced. The recognition processes are performed thanks to function calls to the tATN module.
	\item[tATN:] it interprets multimodal commands and executes them.
	\item[KRL:] it holds the knowledge about the objects of the scene, their features and their links (e.g, the possible interactions between two objects).
\end{description}

An example of multimodal command that could be processed by this system is the following use case: a user points a wheel with his hand and says \textit{"Turn the wheel like this"}, while doing the rotation gesture. The behaviour of the system will be explained by showing the processing of this command. A head tracker is used in order to know the face orientation, data gloves are used for the hand tracking and a speech recorder is used for the voice.

\subsection{Gesture processing}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{res/PrOSA.png}
\caption{\label{fig:PrOSA}Gesture Processing module.}
\end{figure}

The PrOSA module is composed of various entities which are represented on figure \ref{fig:PrOSA}. The purpose of each component is described below:

\paragraph{Actuators} These are the link between the hardware and software sensors and the PrOSA module. They transform "messy" data\footnote{"Messy" means that data is generated on-the-fly, with any order and with possibly different representations.} coming from the sensors into well organized, normalized data, in form of Attribute Sequences (AS). In other word, they provide to the system entry data with a suitable representation for it. For example, for the use-case given in section \ref{sec:overview}, four actuators are used: one for the voice, another gathers and converts information about the head position and the two lasts gather and convert information coming from the data gloves.

\paragraph{Histories} They consist of a list of sets of measures token at regular intervals of time. For example, a speech history is the list of words said by the actor with respect to the time. A spacemap history will rather describe the evolution of a set of distances of objects to the user and angles with the face direction. The histories are necessary for the multimodal analysis module to process formerly acquired data.

\paragraph{Raters} They are the entities that extract the measures to store in the histories from the AS provided by the actuators.

\subsection{Knowledge Representation Layer}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{res/krl.png}
\caption{\label{fig:krl}Knowledge Representation Layer.}
\end{figure}

This module is described on figure \ref{fig:krl}, which is self-explanatory. All instances of objects in the KRL have their counterpart in the scene graph. Such a couple defines a Semantic Entity (SE). A mediator module allows to synchronize different representations of the same SE for different modules. For example, a transition of an object from a position to another can be represented as a difference between two points in the world coordinate system or as a graph path (each node represents a zone of the world).

In our example, the KRL allows the other modules to know what interactions are possible with a wheel and with what speech and gesture command.

\subsection{Multimodal integration module}
\label{subsec:recognition}
The intuition of the recognition module working is to sequentially analyse the user actions (including speech and gestures) step by step, while setting up the resulting action. When the users actions were entirely analysed, the resulting system action is considered to be completely defined and so it is performed.

We can take the use-case defined in section \ref{sec:overview} in order to illustrate this intuition:

\paragraph{"Turn..."} The system "understands" that it will have to perform a rotation and as a consequence sets up a Rotation Action. This Action is characterized by an object to rotate, an axis and an angle, which remain unknowns for now.

\paragraph{Pointing gesture} The object to rotate is located in the direction pointed by the user. As there can be many different objects in this direction, the indications are insufficient for now.

\paragraph{"... the wheel..."} The object is a wheel. The system can now gather the information and use the fact that the direction is known in order to determine which wheel is pointed.

\paragraph{"... like this."} The system prepares now to recognize a movement description. As the awaited movement is a rotation, it will now await an action allowing to define an axis and an angle.

\paragraph{Rotation gesture} From the gesture, the system can retrieve the angle and the axis. The wheel rotates following the gesture, at the same time.
\\
\\
The formal way to implement this intuition is to use a graph structure (as it is done in the tATN module). An example is given on figure \ref{fig:tATN}. The edges are associated to a transition condition (a detected action of the user), guarding a system action (its reaction to the user action, as depicted in the intuition). The nodes don't have a semantic signification, but they can be interpreted as the states of the system control variables used in order to set up the resulting action. The interest of this system is to execute commands "on-the-fly", what provides agility to the system, in opposition to a system which would buffer commands and then execute them.

The node \textit{R42} is in fact a motion modificator or manipulator of the PrOSA module (see figure \ref{fig:PrOSA}), shared with the tATN module. It corresponds to the state in which the wheel rotates following the hand movement. Hence, the command is special because the rotation gesture and the feedback, i.e the rotating wheel, must be executed at the same time. It illustrates the fact that the gesture processing and the interpretation have to be coupled when a continuous feedback is needed. In the opposite case, the interpretation of the gesture could occur too late, because the simulation loop would have to alternate the interpretation of gestures, stored in a buffer for example, and the resulting executions.

Other coupling are used. For instance, a system action can require a call to a function of the PrOSA module, like a predicate linked to the histories, in order to know if the user was facing a particular object for example. The PrOSA module itself can call the tATN module in order to know if a gesture must be interpreted as a special command. Calls can be triggered from the tATN to the KRL too, in order to know what actions are possible on a wheel for instance.

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{res/tATN.png}
\caption{\label{fig:tATN}tATN module.}
\end{figure}

\section{Review}

Although the goal of this paper is obviously not to test a method but to present a tool, some evaluation would be appreciated.

\subsection{Evolution}

The first system introducing multimodality for interaction with a VE is described in \cite{putthatthere}. Its principle is very simple since the objects with which the user can interact are all of the same nature and are displayed on a 2D screen to which the user can point. The paper uses as an example geometrical forms, but we can obviously extend the principle to other objects, like diagrams components for instance. A finite set of possible actions are defined in the system: creation of a form, deletion, motion, modification of the shape and color and renaming. They are triggered by a corresponding English verb that must be said at the beginning of a sentence. For example, \textit{"Move the red circle to the right of that"} triggers a move. The system identifies the form to move thanks to its description, \textit{"the red circle"}. In order to identify what form is referenced by the word \textit{"that"}, the system uses the point of the screen pointed by the user when the word is uttered.

It is similar to the system presented in the current paper, in the sense that the speech is considered as a command. In both cases, the human language is processed as a computer language, with a grammar system. Indeed, the graph structure described in section \ref{subsec:recognition} is nothing less than a grammar analyser coupled with an interpreter.

Obviously, the system described here is richer, since it provides to gestures a stronger semantic power. It not only allows to point an object, but also to describe a movement for instance. Gestures are here part of the control language. Moreover, it solves a problem mentioned in \cite{putthatthere}: when the user wants to rename a form, he has to pause for an instant before uttering the new name of the object, like in the sentence \textit{"Call that... the companion cube"}. This delay time is necessary for the system to switch from the speech recognition mode to the training mode (for the learning of the name "companion cube"). In the current system, such a switching of context is not necessary because the tATN module has only to call one of its own functions in order to learn new utterances. Here, a module is viewed as an object providing several demand services, not as a process needing to operate context changes.

Obviously, it must be recall that \cite{putthatthere} was published in 1980 and that the systems speed has increased since then. However, the "change of context" problem remains actual. The proposed modularity solves this type of issue and so proves its interest.

\subsection{Influence in the scientists community}

The present paper has a certain number of interests that where useful to the VR scientists. They can be categorized:

\begin{thebibliography}{1}
	\bibitem{putthatthere} BOLT, Richard A. \textit{‚ÄúPut-that-there‚Äù: Voice and gesture at the graphics interface.} ACM, 1980.
\end{thebibliography}

\end{document}