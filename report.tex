\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}

\title{Your Paper}

\author{You}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section{Abstract of the paper}

This paper presents a Human Machine multimodal Interface framework for Virtual Environments (VE). The addressed interaction paradigms are based on speech and gesture recognition (multimodal aspect). Its main challenge is to be efficient in a highly interactive context, with an obligation to adapt to real-time. Another requirement is to be an adaptable tool, re-usable, extensible and configurable.

A multimodal interface use recognition methods that must take into account the VE context. For example, in speech recognition, spatial positioning depends on the spatial position of the speaker (\textit{"The left object"} refers to different objects with respect to the user position in the scene). The problem can be solved by implementing a close coupling between the scene representation and the recognition modules. However, such a coupling suffers of 3 main issues (TODO: reformuler apr√®s avoir mieux compris l'article):
\begin{enumerate}
	\item Sampling rates policy different between speech recognition and VR systems.
	\item De-synchronization between the recognition system and the VE. The recognition process needs information on the current state of the scene and so "waits" for new data, while the systems evolves through another state. There is also a need for former information, not present any more in the current state of the scene.
	\item The simulation rate can change.
\end{enumerate}

\section{Overview of the system}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{res/overview.png}
\caption{\label{fig:overview}Architecture of the framework.}
\end{figure}

The figure \ref{fig:overview} shows an overview of the framework architecture. It includes three main modules: \textit{gesture processing} (PrOSA), \textit{multimodal integration and analysis} (tATN) and \textit{Knowledge Representation Layer} (KRL). Their purpose will be explained in details in the latter sections of the report. However, some generalities can be given here:
\begin{description}
	\item[PrOSA:] uses attribute sequences generated by the scene, interpreting them as gestures. The direct effects on the VE are produced. The recognition processes are performed thanks to function calls to the tATN module.
	\item[tATN:] performs the recognition operations. When some context is necessary, it calls function of the two other modules.
	\item[KRL:] holds the knowledge about the objects of the scene, their features and their links (e.g, the possible interactions between two objects).
\end{description}

An example of multimodal command that could be processed by this system is the following use case: a user points a wheel with his hand and says \textit{"Turn the wheel like this"}, while doing the rotation gesture. The behaviour of the system will be explained by showing the processing of this command.

\end{document}