\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}

\title{Your Paper}

\author{You}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section{Abstract of the paper}

This paper presents a Human Machine multimodal Interface framework for Virtual Environments (VE). The addressed interaction paradigms are based on speech and gesture recognition (multimodal aspect). Its main challenge is to be efficient in a highly interactive context, with an obligation to adapt to real-time. Another requirement is to be an adaptable tool, re-usable, extensible and configurable.

A multimodal interface use recognition methods that must take into account the VE context. For example, in speech recognition, spatial positioning depends on the spatial position of the speaker (\textit{"The left object"} refers to different objects with respect to the user position in the scene). The problem can be solved by implementing a close coupling between the scene representation and the recognition modules. However, such a coupling suffers of 3 main issues (TODO: reformuler après avoir mieux compris l'article):
\begin{enumerate}
	\item Sampling rates policies differ between classic speech recognition methods and VR systems.
	\item De-synchronization between the recognition system and the VE. The recognition process needs information on the current state of the scene and so "waits" for new data, while the systems evolves through another state. There is also a need for former information, not present any more in the current state of the scene.
	\item The simulation rate can change.
\end{enumerate}

\subsection{Overview of the system}
\label{sec:overview}
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{res/overview.png}
\caption{\label{fig:overview}Architecture of the framework.}
\end{figure}

The figure \ref{fig:overview} shows an overview of the framework architecture. It includes three main modules: \textit{gesture processing} (PrOSA), \textit{multimodal integration and analysis} (tATN) and \textit{Knowledge Representation Layer} (KRL). Their purpose will be explained in details in the latter sections of the report. However, some generalities can be given here:
\begin{description}
	\item[PrOSA:] it uses attribute sequences generated by the scene, interpreting them as gestures. The direct effects on the VE are produced. The recognition processes are performed thanks to function calls to the tATN module.
	\item[tATN:] it interprets multimodal commands and executes them.
	\item[KRL:] it holds the knowledge about the objects of the scene, their features and their links (e.g, the possible interactions between two objects).
\end{description}

An example of multimodal command that could be processed by this system is the following use case: a user points a wheel with his hand and says \textit{"Turn the wheel like this"}, while doing the rotation gesture. The behaviour of the system will be explained by showing the processing of this command. A head tracker is used in order to know the face orientation, data gloves are used for the hand tracking and a speech recorder is used for the voice.

\subsection{Gesture processing}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{res/PrOSA.png}
\caption{\label{fig:PrOSA}Gesture Processing module.}
\end{figure}

The PrOSA module is composed of various entities which are represented on figure \ref{fig:PrOSA}. The purpose of each component is described below:

\paragraph{Actuators} These are the link between the hardware and software sensors and the PrOSA module. They transform "messy" data\footnote{"Messy" means that data is generated on-the-fly, with any order and with possibly different representations.} coming from the sensors into well organized, normalized data, in form of Attribute Sequences (AS). In other word, they provide to the system entry data with a suitable representation for it. For example, for the use-case given in section \ref{sec:overview}, four actuators are used: one for the voice, another gathers and converts information about the head position and the two lasts gather and convert information coming from the data gloves.

\paragraph{Histories} They consist of a list of sets of measures token at regular intervals of time. For example, a speech history is the list of words said by the actor with respect to the time. A spacemap history will rather describe the evolution of a set of distances of objects to the user and angles with the face direction. The histories are necessary for the multimodal analysis module to process formerly acquired data.

\paragraph{Raters} They are the entities that extract the measures to store in the histories from the AS provided by the actuators.

\subsection{Knowledge Representation Layer}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{res/krl.png}
\caption{\label{fig:krl}Knowledge Representation Layer.}
\end{figure}

This module is described on figure \ref{fig:krl}, which is self-explanatory. All instances of objects in the KRL have their counterpart in the scene graph. Such a couple defines a Semantic Entity (SE). A mediator module allows to synchronize different representations of the same SE for different modules. For example, a transition of an object from a position to another can be represented as a difference between two points in the world coordinate system or as a graph path (each node represents a zone of the world).

In our example, the KRL allows the other modules to know what interactions are possible with a wheel and with what speech and gesture command.

\subsection{Multimodal integration module}
\label{subsec:recognition}
The intuition of the recognition module working is to sequentially analyse the user actions (including speech and gestures) step by step, while setting up the resulting action. When the users actions were entirely analysed, the resulting system action is considered to be completely defined and so it is performed.

We can take the use-case defined in section \ref{sec:overview} in order to illustrate this intuition:

\paragraph{"Turn..."} The system "understands" that it will have to perform a rotation and as a consequence sets up a Rotation Action. This Action is characterized by an object to rotate, an axis and an angle, which remain unknowns for now.

\paragraph{Pointing gesture} The object to rotate is located in the direction pointed by the user. As there can be many different objects in this direction, the indications are insufficient for now.

\paragraph{"... the wheel..."} The object is a wheel. The system can now gather the information and use the fact that the direction is known in order to determine which wheel is pointed.

\paragraph{"... like this."} The system prepares now to recognize a movement description. As the awaited movement is a rotation, it will now await an action allowing to define an axis and an angle.

\paragraph{Rotation gesture} From the gesture, the system can retrieve the angle and the axis. The wheel rotates following the gesture, at the same time.
\\
\\
The formal way to implement this intuition is to use a graph structure (as it is done in the tATN module). An example is given on figure \ref{fig:tATN}. The edges are associated to a transition condition (a detected action of the user), guarding a system action (its reaction to the user action, as depicted in the intuition). The nodes don't have a semantic signification, but they can be interpreted as the states of the system control variables used in order to set up the resulting action. The interest of this system is to execute commands "on-the-fly", what provides agility to the system, in opposition to a system which would buffer commands and then execute them.

The node \textit{R42} is in fact a motion-modificator or manipulator of the PrOSA module (see figure \ref{fig:PrOSA}), shared with the tATN module. It corresponds to the state in which the wheel rotates following the hand movement. Hence, the command is special because the rotation gesture and the feedback, i.e the rotating wheel, must be executed at the same time. It illustrates the fact that the gesture processing and the interpretation have to be coupled when a continuous feedback is needed. In the opposite case, the interpretation of the gesture could occur too late, because the simulation loop would have to alternate the interpretation of gestures, stored in a buffer for example, and the resulting executions.

Other coupling are used. For instance, a system action can require a call to a function of the PrOSA module, like a predicate linked to the histories, in order to know if the user was facing a particular object for example. The PrOSA module itself can call the tATN module in order to know if a gesture must be interpreted as a special command. Calls can be triggered from the tATN to the KRL too, in order to know what actions are possible on a wheel for instance.

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{res/tATN.png}
\caption{\label{fig:tATN}tATN module.}
\end{figure}

\section{Brought improvements}

\subsection{Context}
The first system introducing multimodality for interaction with a VE is described in \cite{putthatthere}. Its principle is very simple since the objects with which the user can interact are all of the same nature and are displayed on a 2D screen to which the user can point. The paper uses as an example geometrical forms, but we can obviously extend the principle to other objects, like diagrams components for instance. A finite set of possible actions are defined in the system: creation of a form, deletion, motion, modification of the shape and color and renaming. They are triggered by a corresponding English verb that must be said at the beginning of a sentence. For example, \textit{"Move the red circle to the right of that"} triggers a move. The system identifies the form to move thanks to its description, \textit{"the red circle"}. In order to identify what form is referenced by the word \textit{"that"}, the system uses the point of the screen pointed by the user when the word is uttered.

It is similar to the system presented in the current paper, in the sense that the speech is considered as a command. In both cases, the human language is processed as a computer language, with a grammar system. Indeed, the graph structure described in section \ref{subsec:recognition} is nothing less than a grammar analyser coupled with an interpreter.

\subsection{New interaction possibilities}
\label{subsec:possibilities}

\begin{figure}
\centering
\includegraphics[width=1.25\textwidth]{res/kaiser_architecture.png}
\caption{\label{fig:kaiser}Architecture presented in \cite{kaiser}.}
\end{figure}

In comparison to the pioneer system presented in \cite{putthatthere}, the system described here is richer, since it provides to gestures a stronger semantic power. It not only allows to point an object, but also to describe a movement for instance. Gestures are here part of the control language.

Gesture processing and execution of commands being highly coupled, it is possible to provide continuous feedback, i.e performed at the same time than the gesture, like in our first example of the rotating wheel. It is one of the main enhancements brought by the system. In comparison, a former architecture presented at figure \ref{fig:kaiser}, still young when the paper was published\footnote{respective years of publication: 2003 and 2005}, presented a complete decoupling between the gesture processing and the multimodal integration. Indeed, the message passing system form a closed loop where the gestures and utterances transit as a command flow, interpreted when the next involved module is ready. It introduces delays that are not compatible with continuous feedback. As a matter of fact, the examples of interactions reported in \cite{kaiser} show users imitating a motion (e.g, rotation) for the object to move as a consequence of the complete gesture but never show users controlling motions "online".

\subsection{Shorten delays}
\label{subsec:delays}
It is hard to make comparisons between this system an others in terms of delays because of the absence of any evaluation. Although the goal of this paper is obviously not to test a method but to present a tool, some evaluation would have been appreciated. Still, some assumptions can be made, considering that the enhancements claimed by the author are actual.

The current presented system solves a problem mentioned in \cite{putthatthere}: when the user wants to rename a form, he has to pause for an instant before uttering the new name of the object, like in the sentence \textit{"Call that... the companion cube"}. This delay time is necessary for the system to switch from the speech recognition mode to the training mode (for the learning of the name "companion cube"). In the current system, such a switching of context is not necessary because the KRL module allows to define the possible interaction "Recall" on some objects, parametrized with specific utterances. Here, a simple use of the object-centered concept avoids such delays. Obviously, it must be recall that \cite{putthatthere} was published in 1980 and that the systems speed has increased since then. However, the "change of context" problem remains actual. The proposed modularity solves this type of issue and so proves its interest.

As a direct consequence of what is said in section \ref{subsec:possibilities} about message based communication between the modules of the architecture presented in \cite{kaiser}, one can expect reduced delays between the ending of a gesture or sentence utterance and the visual feedback. The delay presented in \cite{kaiser} of 1.1 seconds in average is not high but perceptible and more importantly can make the user feel that he doesn't really interact with the manipulated object, as stated in \cite{responsetime}. It excludes the integration of continuous feedback.

\subsection{Multimodal input fusion}

The Multimodal Integration module is finite-state based. This approach was first introduced in \cite{state-based}. It presents how multimodal interactions composed of gestures and voice can be processed by a context-free grammar, that can be represented as a graph where conditions guard the transitions and corresponding actions, in the same way than it is presented in section \ref{subsec:recognition}. The conditions are in fact composed of two elements: a gesture and a word command, possibly empty. The tATN can be also specified this way. Such a graph is called a finite-state transducer (FST).

The main conccurent approach is unification-based. It consists in...

The advantages of the state-based approach are...

When a multimodal command is executed, the graph is first "constrained" by the gesture sequence: first, all the translations are filtered by the gesture sequence, in order to guide the speech recognition. More formally, the transducer translation function is $\tau:(G \leftarrow S) \leftarrow M$, where $G$ is the gesture set, $S$ is the word command set and $M$ is the action set. This results on the definition of a transducer representing the relation between $G$ and $S$. Then, the resulting multimodal command processing steps are:
\begin{enumerate}
	\item ...
\end{enumerate}
%(insert a figure or two)
However, the proposed processing in the current paper differs from the previous one in the sense that...(timestamp allowing..., integration of more complex gestures thanks to the PrOSA module, description of more complex objects thanks to the KRL module).

\section{Review}

\subsection{Immersion impression}
The paper has the main benefit of developing the concept of post-WIMP interfaces, i.e interfaces giving a pleasant impression of immersion to the user. Obviously, post-WIMP is a fundamental idea in VR. Nevertheless, one can distinguish several degrees of immersion for a same interaction. For instance, in \cite{avatars}, the authors compare several ways to show that an exchange of avatars is happening. One of them consists in a simple popup notification whereas another shows ghosts translating between the two avatars. the first metaphor is a kind of WIMP equivalent, while the second one complies with the post-WIMP concept. However, post-WIMP metaphors are not necessarily better than WIMP ones, and the results in \cite{avatars} confirm it. Moreover, button based commands are nowadays widely used (e.g, video game remotes) and cheaper than motion trackers. Therefore, their usage shouldn't be deprecated.

In training applications, the immersion impression brought by multimodality is fundamental because it is a pleasant way to interact that stimulates the users, who feel more involved in the task, as it is claimed in \cite{eutap}. 

\subsection{Tight coupling}
The coupling of the modules, due to possible function calls between the tATN module and the others and also due to the motion-modificators and manipulators shared by the PrOSA and the tATN modules, has the main benefit of removing delays, as explained in sections \ref{subsec:recognition} and \ref{subsec:delays}. However, it brings also too many dependencies between the modules and as a result affects the re-usability and the customizability of the code, what the author himself admits in \cite{hcii}. He proposes as an alternative to substitute function calls by functors. Indeed, these objects inherit from the top-class of the programming language used (the \texttt{Object} class in java) and so guarantee the re-usability of the algorithms. The programmers only have to encapsulate all their own implementations as functors in the same specific module. 

\begin{thebibliography}{1}
	\bibitem{putthatthere} BOLT, Richard A. \textit{“Put-that-there”: Voice and gesture at the graphics interface.} ACM, 1980.
	\bibitem{hcii} LATOSCHIK, Marc Erich et FISCHBACH, Martin. \textit{Engineering Variance: Software Techniques for Scalable, Customizable, and Reusable Multimodal Processing.}
	\bibitem{avatars} LOPEZ, Thomas, BOUVILLE, Rozenn, LOUP-ESCANDE, Emilie, et al. \textit{Exchange of avatars: Toward a better perception and understanding. Visualization and Computer Graphics, IEEE Transactions on}, 2014, vol. 20, no 4, p. 644-653.
	\bibitem{eutap} HEGARTY, Rosaleen, CONLON, Sonya, et BLYTH, Elijah. European Union Training Application (EU Tap).
	\bibitem{kaiser} KAISER, Ed, OLWAL, Alex, MCGEE, David, et al. Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality. In : Proceedings of the 5th international conference on Multimodal interfaces. ACM, 2003. p. 12-19.
	\bibitem{responsetime} Nielsen Norman Group, \textit{Response time limits} [online]. Access: \url{http://www.nngroup.com/articles/response-times-3-important-limits/}. [Accessed: 1st of January, 2015].
	\bibitem{state-based} JOHNSTON, Michael et BANGALORE, Srinivas. Finite-state multimodal parsing and understanding. In : \textit{Proceedings of the 18th conference on Computational linguistics-Volume 1}. Association for Computational Linguistics, 2000. p. 369-375.
\end{thebibliography}

\end{document}